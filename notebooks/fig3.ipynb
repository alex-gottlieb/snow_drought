{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os \n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterio import features\n",
    "from affine import Affine\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl import geoaxes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from scipy.stats import t, norm, ttest_1samp, gaussian_kde\n",
    "import xesmf as xe\n",
    "from time import time\n",
    "import warnings\n",
    "from random import choices, sample, choice\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rasterize(shapes, coords, fill=np.nan, **kwargs):\n",
    "    \"\"\"Rasterize a list of (geometry, fill_value) tuples onto the given\n",
    "    xarray coordinates. This only works for 1d latitude and longitude\n",
    "    arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    def transform_from_latlon(lat, lon):\n",
    "        lat = np.asarray(lat)\n",
    "        lon = np.asarray(lon)\n",
    "        trans = Affine.translation(lon[0], lat[0])\n",
    "        scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "        return trans * scale\n",
    "\n",
    "    transform = transform_from_latlon(coords['lat'], coords['lon'])\n",
    "    out_shape = (len(coords['lat']), len(coords['lon']))\n",
    "    raster = features.rasterize(shapes, out_shape=out_shape,\n",
    "                                fill=fill, transform=transform,\n",
    "                                dtype=float, **kwargs)\n",
    "    return xr.DataArray(raster, coords=coords, dims=('lat', 'lon'))\n",
    "\n",
    "def get_dowy(date):\n",
    "    if date is None:\n",
    "        return np.nan\n",
    "    date = pd.to_datetime(date)\n",
    "    if date.month >= 10:\n",
    "        dowy = date.dayofyear - pd.to_datetime(f\"{date.year}-10-01\").dayofyear + 1\n",
    "    else: \n",
    "        dowy = date.dayofyear + 92\n",
    "    if date.year % 4 == 0 and dowy>=92+61:\n",
    "        dowy = dowy -1\n",
    "    return dowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/dartfs-hpc/rc/lab/C/CMIG\"\n",
    "data_dir = os.path.join(root_dir, \"agottlieb\",'git_repos',\"snow_drought\", \"data\")\n",
    "basin_dir = os.path.join(data_dir,'basin')\n",
    "fig_dir = os.path.join(root_dir, \"agottlieb\",'git_repos',\"snow_drought\", \"figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdc_basins = gpd.read_file(os.path.join(data_dir, \"boundaries\", \"grdc_basins\"), layer=\"mrb_basins\")\n",
    "basin_ids = {k: i for i, k in enumerate(grdc_basins.RIVER_BASI)}\n",
    "basin_ids_rev = {v:k for k,v in basin_ids.items()}\n",
    "basin_shapes = [(shape, n) for n, shape in enumerate(grdc_basins.geometry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_ensemble = []\n",
    "for prod in os.listdir(basin_dir):\n",
    "    prod_dir = os.path.join(basin_dir,prod)\n",
    "    prod_files = [os.path.join(prod_dir,f) for f in os.listdir(prod_dir)]\n",
    "    prod_files.sort()\n",
    "    if prod == 'SNOTEL':\n",
    "        continue\n",
    "    prod_ds = xr.concat([xr.open_dataset(f) for f in prod_files],dim='time').resample(time='1D').mean()\n",
    "    prod_ds = prod_ds.assign_coords(product=prod)\n",
    "    basin_ensemble.append(prod_ds)\n",
    "    print(prod)\n",
    "basin_ensemble = xr.concat(basin_ensemble,dim='product')\n",
    "ens_mean = basin_ensemble.mean(\"product\")\n",
    "ens_mean = ens_mean.assign_coords(product='Ens. mean')\n",
    "basin_ensemble = xr.concat([basin_ensemble,ens_mean],dim='product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_ensemble = basin_ensemble.sel(time=basin_ensemble['time'][~((basin_ensemble['time.month']==2)&(basin_ensemble['time.day']==29))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod='SNOTEL'\n",
    "prod_dir = os.path.join(basin_dir,prod)\n",
    "prod_files = [os.path.join(prod_dir,f) for f in os.listdir(prod_dir)]\n",
    "prod_files.sort()\n",
    "prod_files = prod_files[-18:-2]\n",
    "snotel_ds = xr.open_mfdataset(prod_files).load()\n",
    "snotel_ds = snotel_ds.rename({\"SWE\":\"SNOMAS\"})\n",
    "snotel_ds = snotel_ds.assign_coords(product='SNOTEL')\n",
    "\n",
    "basin_ensemble = basin_ensemble.sel(basin=snotel_ds['basin'].values)\n",
    "basin_ensemble = xr.concat([basin_ensemble,snotel_ds[['SNOMAS']]],dim='product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_metrics = basin_ensemble['SNOMAS'].resample(time='AS-OCT').max().to_dataset(name='peak_swe')\n",
    "sd_metrics['apr1_swe'] = basin_ensemble['SNOMAS'].sel(time=basin_ensemble['time'][(basin_ensemble['time.month']==4)&(basin_ensemble['time.day']==1)]).resample(time='AS-OCT').mean()\n",
    "swei = basin_ensemble['SNOMAS'].rolling(time=90).sum().groupby(\"time.dayofyear\").apply(lambda x: norm.ppf((x.rank(\"time\")-0.44)/(x.count(\"time\")+0.12)))\n",
    "sd_metrics['swei'] = swei.sel(time=swei['time'][swei['time.season']=='MAM']).resample(time='AS-OCT').min()\n",
    "diff_clim = basin_ensemble['SNOMAS'].groupby(\"time.dayofyear\").apply(lambda x: x-x.median())\n",
    "sd_metrics['swe_deficit'] = diff_clim.sel(time=diff_clim['time'][diff_clim['time.season']=='MAM']).resample(time='AS-OCT').sum(min_count=1)\n",
    "\n",
    "peak_dates = basin_ensemble['SNOMAS'].resample(time='AS-OCT').apply(lambda x: x.idxmax(\"time\"))\n",
    "peak_dates = xr.apply_ufunc(get_dowy,peak_dates,input_core_dims=[[]],output_core_dims=[[]],vectorize=True)\n",
    "sd_metrics['peak_date'] = peak_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd_metrics = sd_metrics.to_dataframe().reset_index()\n",
    "\n",
    "grid = xr.Dataset({'lat':np.arange(0.25,90.25,0.5),\n",
    "                   'lon':np.arange(-179.75,180.25,0.5)})\n",
    "basin_grid = rasterize(basin_shapes,coords=grid.coords)\n",
    "basin_grid.name='basin'\n",
    "basin_grid = basin_grid.to_dataframe().reset_index()\n",
    "\n",
    "sd_metrics = sd_metrics.merge(basin_grid,on='basin',how='outer')\n",
    "sd_metrics = xr.Dataset.from_dataframe(sd_metrics.set_index([\"product\",\"time\",\"lat\",\"lon\"]))\n",
    "sd_metrics.to_netcdf(os.path.join(data_dir,'basin_snow_drought_metrics.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(data_dir,'prism.nc')):\n",
    "    prism_dir = os.path.join(root_dir,'Data','Observations','PRISM')\n",
    "    prism_tmean_files = [os.path.join(prism_dir,'tmean',f) for f in os.listdir(os.path.join(prism_dir,'tmean'))]\n",
    "    prism_tmean_files.sort()\n",
    "\n",
    "    prism_ppt_files = [os.path.join(prism_dir,'ppt',f) for f in os.listdir(os.path.join(prism_dir,'ppt'))]\n",
    "    prism_ppt_files.sort()\n",
    "\n",
    "    prism_tmean = xr.open_mfdataset(prism_tmean_files[:-3])\n",
    "\n",
    "    # average temperature from first half of WY\n",
    "    prism_tmean_ondjfm = prism_tmean.sel(time=prism_tmean['time'][(prism_tmean['time.month']>=10)|(prism_tmean['time.month']<=3)]).resample(time='AS-OCT').mean()\n",
    "\n",
    "    prism_ppt = xr.open_mfdataset(prism_ppt_files[:-1])\n",
    "    prism_ppt = prism_ppt.rename({'__xarray_dataarray_variable__':'ppt'})\n",
    "    prism_ppt['lat'] = prism_tmean['lat']\n",
    "    prism_ppt['lon'] = prism_tmean['lon']\n",
    "\n",
    "    # cumulative precipitation from first half of WY\n",
    "    prism_ppt_ondjfm = prism_ppt.sel(time=prism_ppt['time'][(prism_ppt['time.month']>=10)|(prism_ppt['time.month']<=3)]).resample(time=\"AS-OCT\").sum()\n",
    "    prism_tmean_ondjfm['ppt'] = prism_ppt_ondjfm['ppt']\n",
    "\n",
    "    ds_out = xr.Dataset({'lat':np.arange(0.25,90.25,0.5),\n",
    "                         'lon':np.arange(-179.75,180.25,0.5)})\n",
    "    regridder = xe.Regridder(prism_tmean_ondjfm,ds_out,'bilinear')\n",
    "    prism_regrid = regridder(prism_tmean_ondjfm)\n",
    "\n",
    "    prism_regrid.to_netcdf(os.path.join(data_dir,'prism.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism = xr.open_dataset(os.path.join(data_dir,'prism.nc'))\n",
    "# add PRISM data to SWE dataset\n",
    "sd_metrics = sd_metrics.assign(tmean=(('time','lat','lon'),prism['tmean'].sel(time=sd_metrics['time'])),\n",
    "                               ppt=(('time','lat','lon'),prism['ppt'].sel(time=sd_metrics['time'])))\n",
    "\n",
    "# clip to CONUS\n",
    "sd_metrics = sd_metrics.sel(lat=slice(24,50),lon=slice(-125,-66))\n",
    "\n",
    "# calculate T and P anomalies relative to climatology\n",
    "sd_metrics['t_anom'] = sd_metrics['tmean']-sd_metrics['tmean'].mean(\"time\")\n",
    "sd_metrics['p_anom'] = sd_metrics['ppt']/sd_metrics['ppt'].median(\"time\")\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdm = xr.open_dataset(os.path.join(data_dir,'usdm.nc'))\n",
    "usdm[\"drought_class\"] = usdm[\"drought_class\"].fillna(-1)\n",
    "usdm = usdm.where(usdm.sum(\"time\")!= -len(usdm[\"time\"]))\n",
    "usdm[\"onset_intensification\"] = (usdm[\"drought_class\"] - usdm[\"drought_class\"].shift(time=1)) # identify state changes in drought\n",
    "usdm[\"onset_intensification\"] = usdm[\"onset_intensification\"].clip(max=1) # clip to 1 to make binary (in case increase by >=2)\n",
    "\n",
    "usdm_max = usdm.where((usdm[\"time.month\"]>=4) & (usdm[\"time.month\"]<=9)).resample(time=\"AS-OCT\").max() # most severe AMJJAS drought class\n",
    "usdm_max['drought_bin'] = ((usdm_max[\"drought_class\"]>=1) & (usdm_max[\"onset_intensification\"]==1)).astype(int)\n",
    "sd_metrics = xr.merge([sd_metrics,usdm_max],join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ets(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Equitable threat score metric\n",
    "    \"\"\"\n",
    "    hits = (y_true[y_pred==1]).sum()\n",
    "    misses = (y_true[y_pred==0]).sum()\n",
    "    false_alarms = (y_pred[y_true==0]).sum()\n",
    "    correct_rejections = ((y_pred==0) & (y_true==0)).astype(int).sum()\n",
    "    E = ((hits+misses)*(hits+false_alarms))/len(y_true)\n",
    "    ets = (hits-E)/(hits+misses+false_alarms-E)\n",
    "    return ets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get basins that contain SNOTEL sites\n",
    "with xr.open_dataset(os.path.join(root_dir,'Data','Observations','SNOTEL','snotel_basin_yearly_stats.nc')) as ds:\n",
    "    snotel_basins = ds['basin']\n",
    "    map_dict = dict(zip(grdc_basins['MRBID'],grdc_basins.index))\n",
    "    snotel_basins = np.vectorize(map_dict.get)(snotel_basins.values)\n",
    "    snotel_basins = snotel_basins[snotel_basins!=basin_ids['NELSON']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_data = [sd_metrics.where(sd_metrics['basin']==b) for b in snotel_basins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basin_null(basin_data):\n",
    "    \"\"\"\n",
    "    Get 10x 5-fold cross-validation estimates of forecast skill from null model (T & P only)\n",
    "    \"\"\"\n",
    "    stacked = basin_data.stack(z=(\"time\",\"lat\",\"lon\"))\n",
    "    X1 = stacked['t_anom'].values.reshape(-1,1)\n",
    "    X2 = stacked['p_anom'].values.reshape(-1,1)\n",
    "    X = np.concatenate([X1,X2],axis=1)\n",
    "    y = stacked['drought_bin'].values\n",
    "    \n",
    "    X = X[~np.isnan(y),:]\n",
    "    y = y[~np.isnan(y)]\n",
    "    X = np.nan_to_num(X,0.5)\n",
    "    \n",
    "    rkf = RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
    "    lr = LogisticRegression()\n",
    "    ets_cv = cross_val_score(lr,X,y,scoring=make_scorer(ets),cv=rkf,n_jobs=-1,verbose=1)\n",
    "\n",
    "    return ets_cv\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir,'ets','basin_ets_null.csv')):\n",
    "    pool = Pool(10)\n",
    "    res = pool.map(basin_null,basin_data)\n",
    "    null_df = pd.DataFrame(res,index=snotel_basins)\n",
    "    null_df.index.name = 'basin'\n",
    "    null_df.to_csv(os.path.join(data_dir,'ets','basin_ets_null.csv'))\n",
    "    pool.close()\n",
    "else:\n",
    "    null_df = pd.read_csv(os.path.join(data_dir,'ets','basin_ets_null.csv')).set_index(\"basin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snow drought classifications based on definitions from the literature\n",
    "sd_metrics['peak_swe_anom'] = (sd_metrics['peak_swe']-sd_metrics['peak_swe'].mean(\"time\"))/sd_metrics['peak_swe'].std(\"time\")\n",
    "sd_metrics['apr1_swe_anom'] = (sd_metrics['apr1_swe']-sd_metrics['apr1_swe'].mean(\"time\"))/sd_metrics['apr1_swe'].std(\"time\")\n",
    "sd_metrics['peak_date_anom'] = (sd_metrics['peak_date']-sd_metrics['peak_date'].mean(\"time\"))/sd_metrics['peak_date'].std(\"time\")\n",
    "\n",
    "\n",
    "# Huning & AghaKouchak (2020): SWEI < -0.8 (continuous)\n",
    "sd_ha20 = (sd_metrics[\"swei\"] < -0.8).astype(int).where(~xr.ufuncs.isnan(sd_metrics[\"swei\"]))\n",
    "sd_ha20.name = \"snow_drought\"\n",
    "sd_ha20 = sd_ha20.assign_coords(definition=\"ha20\")\n",
    "\n",
    "# Harpold et al. (2017): < clim. Apr. 1 SWE \n",
    "sd_h17 = (sd_metrics[\"apr1_swe_anom\"] < 0).astype(int).where(~xr.ufuncs.isnan(sd_metrics[\"apr1_swe_anom\"]))\n",
    "sd_h17.name = \"snow_drought\"\n",
    "sd_h17 = sd_h17.assign_coords(definition=\"h17\")\n",
    "\n",
    "# Marshall et al. (2019): < 25th %ile peak SWE\n",
    "sd_m19_low = (sd_metrics[\"peak_swe_anom\"]<norm.ppf(0.25)).astype(int).where(~xr.ufuncs.isnan(sd_metrics[\"peak_swe_anom\"]))\n",
    "sd_m19_low.name = \"snow_drought\"\n",
    "sd_m19_low = sd_m19_low.assign_coords(definition=\"m19_low\")\n",
    "\n",
    "# Marshall et al. (2019): < 25th %ile peak SWE date\n",
    "sd_m19_early = (sd_metrics[\"peak_date_anom\"]<norm.ppf(0.25)).astype(int).where(~xr.ufuncs.isnan(sd_metrics[\"peak_date_anom\"]))\n",
    "sd_m19_early.name = \"snow_drought\"\n",
    "sd_m19_early = sd_m19_early.assign_coords(definition=\"m19_early\")\n",
    "\n",
    "# Dierauer et al. (2019): < clim. peak SWE\n",
    "sd_d19 = (sd_metrics[\"peak_swe_anom\"]<0).astype(int).where(~xr.ufuncs.isnan(sd_metrics[\"peak_swe_anom\"]))\n",
    "sd_d19.name=\"snow_drought\"\n",
    "sd_d19 = sd_d19.assign_coords(definition=\"d19\")\n",
    "\n",
    "# Hatchett & McEvoy (2017): < clim. SWE (continuous)\n",
    "sd_hm17 = (sd_metrics[\"swe_deficit\"]<0).astype(int).where(~xr.ufuncs.isnan(sd_metrics[\"swe_deficit\"]))\n",
    "sd_hm17.name=\"snow_drought\"\n",
    "sd_hm17 = sd_d19.assign_coords(definition=\"hm17\")\n",
    "\n",
    "# combine all definitions \n",
    "def_ds = xr.concat([sd_ha20, sd_h17, sd_m19_low, sd_m19_early, sd_d19, sd_hm17],dim=\"definition\")\n",
    "\n",
    "# merge snow drought and USDM datasets\n",
    "def_ds = def_ds.sel(lat=slice(24,50),lon=slice(-125,-66)) # clip to US\n",
    "def_ds = xr.merge([def_ds,usdm_max],join='inner')\n",
    "data_def_combos = def_ds.drop_sel(product='Ens. mean',).stack(data_def=(\"definition\",\"product\")) # make data-definition combos single dimension\n",
    "data_def_combos['tmean'] = sd_metrics['tmean']\n",
    "data_def_combos['ppt'] = sd_metrics['ppt']\n",
    "data_def_combos['t_anom'] = sd_metrics['t_anom']\n",
    "data_def_combos['p_anom'] = sd_metrics['p_anom']\n",
    "data_def_combos['basin'] = sd_metrics['basin'].mean(\"product\").mean(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basin_data_def(data_def,basin,data):\n",
    "    \"\"\"\n",
    "    Get 10x 5-fold cross-validation estimates of forecast skill from model that includes each individual dataset-definition combination, as well as T & P\n",
    "    \"\"\"\n",
    "    basin_data = data.sel(data_def=data_def).where(data['basin']==basin).stack(z=('time','lat','lon'))\n",
    "    X1 = basin_data['t_anom'].values.reshape(-1,1)\n",
    "    X2 = basin_data['p_anom'].values.reshape(-1,1)\n",
    "    X3 = basin_data['snow_drought'].values.reshape(-1,1)\n",
    "    X = np.concatenate([X1,X2,X3],axis=1)\n",
    "    y = basin_data['drought_bin'].values\n",
    "    \n",
    "    X = X[~np.isnan(y),:]\n",
    "    y = y[~np.isnan(y)]\n",
    "    X = np.nan_to_num(X,0.5)\n",
    "    \n",
    "    rkf = RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
    "    lr = LogisticRegression()\n",
    "    ets_cv = cross_val_score(lr,X,y,scoring=make_scorer(ets),cv=rkf,n_jobs=-1,verbose=1)\n",
    "    return ets_cv\n",
    "\n",
    "# get cross-val estimates of ETS for each dataset-definition combination in each basin, save to separate file\n",
    "for b in np.sort(snotel_basins):\n",
    "    if not os.path.isdir(os.path.join(data_dir,'ets','basin_ets')):\n",
    "        os.makedirs(os.path.join(data_dir,'ets','basin_ets'))\n",
    "    if not os.path.exists(os.path.join(data_dir,'ets','basin_ets',f'{basin_ids_rev[b]}.csv')):\n",
    "        pool = Pool(8)\n",
    "        p_func = partial(basin_data_def,basin=b,data=data_def_combos)\n",
    "        res = pool.map(p_func,data_def_combos['data_def'].values)\n",
    "        basin_df = pd.DataFrame(res,index=data_def_combos['data_def'].values)\n",
    "        basin_df['basin'] = b\n",
    "        basin_df.index = pd.MultiIndex.from_tuples(basin_df.index,names=['definition','product'])\n",
    "        basin_df = basin_df.reset_index().set_index(['basin','definition','product'])\n",
    "        basin_df.to_csv(os.path.join(data_dir,'ets','basin_ets',f'{basin_ids_rev[b]}.csv'))\n",
    "        pool.close()\n",
    "        print(b, \"complete\")\n",
    "    else:\n",
    "        print(b, 'already complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all separate basin files into one DataFrame\n",
    "basin_df = pd.concat([pd.read_csv(os.path.join(data_dir,'ets','basin_ets',f'{basin_ids_rev[b]}.csv')).set_index([\"basin\",\"definition\",\"product\"]) for b in snotel_basins])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median estimate of forecast skill from all cross-validation estimates for each data-def combo in each basin\n",
    "ets_med = basin_df.median(axis=1).reset_index().rename(columns={0:'ets'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basin_ens_mean(basin_data,ens_mean):\n",
    "    \"\"\"\n",
    "    Get 10x 5-fold cross-validation estimates of forecast skill from model that includes ensemble mean (weighted by skill of members), as well as T & P\n",
    "    \"\"\"\n",
    "    stacked = basin_data.stack(z=(\"time\",\"lat\",\"lon\"))\n",
    "    X1 = stacked['t_anom'].values.reshape(-1,1)\n",
    "    X2 = stacked['p_anom'].values.reshape(-1,1)\n",
    "    X3 = ens_mean.stack(z=(\"time\",\"lat\",'lon')).values.reshape(-1,1)\n",
    "    X = np.concatenate([X1,X2,X3],axis=1)\n",
    "    y = stacked['drought_bin'].values\n",
    "    \n",
    "    X = X[~np.isnan(y),:]\n",
    "    y = y[~np.isnan(y)]\n",
    "    X = np.nan_to_num(X,0.5)\n",
    "    \n",
    "    rkf = RepeatedStratifiedKFold(n_splits=5,n_repeats=10,random_state=42)\n",
    "    lr = LinearRegression()\n",
    "    ets_cv = cross_val_score(lr,X,y,scoring=make_scorer(ets),cv=rkf,n_jobs=-1,verbose=1)\n",
    "    return ets_cv\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir,'ets','basin_ets_weighted.csv')):\n",
    "    # convert ETS estimates into xarray Dataset for weighting by skill\n",
    "    dd_ds = []\n",
    "    for dd in ets_med.groupby(['definition','product']).size().index:\n",
    "        d = dd[0]\n",
    "        p = dd[1]\n",
    "        merged = grdc_basins.merge(ets_med[(ets_med['definition']==d)&(ets_med['product']==p)],left_index=True,right_on='basin')\n",
    "        ets_da = rasterize(zip(merged['geometry'],merged['ets']),dict(lat=basin_swe_ensemble['lat'],lon=basin_swe_ensemble['lon']))\n",
    "        ets_da.name='ets'\n",
    "        ets_da = ets_da.assign_coords(definition=d,product=p)\n",
    "        dd_ds.append(ets_da)\n",
    "    dd_ds = xr.concat(dd_ds,dim='data_def')\n",
    "\n",
    "    # ensemble mean snow drought classification weighted by skill of members\n",
    "    weights = dd_ds.copy()\n",
    "    weights.name = 'weights'\n",
    "    sd_weighted = data_def_combos['snow_drought'].weighted(weights.fillna(0)).mean(\"data_def\")\n",
    "\n",
    "\n",
    "    basin_data_defs = [data_def_combos.where(data_def_combos['basin']==b) for b in snotel_basins]\n",
    "    pool = Pool(10)\n",
    "    p_func = partial(basin_ens_mean,ens_mean=sd_weighted)\n",
    "    res = pool.map(p_func,basin_data_defs)\n",
    "    weighted_df = pd.DataFrame(res,index=snotel_basins)\n",
    "    weighted_df.index.name = 'basin'\n",
    "    weighted_df.to_csv(os.path.join(data_dir,'ets','basin_ets_weighted.csv'))\n",
    "    pool.close()\n",
    "else:\n",
    "    weighted_df = pd.read_csv(os.path.join(data_dir,'ets','basin_ets_weighted.csv')).set_index(\"basin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median estimate of forecast skill from all cross-validation estimates for null model in each basin\n",
    "null_med = null_df.median(axis=1)\n",
    "null_med.name = 'null_ets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get % change in forecast skill relative to null modelets_med = ets_med.merge(null_med,left_on='basin',right_index=True)\n",
    "ets_med = ets_med.merge(null_med,left_on='basin',right_index=True)\n",
    "ets_med['pct_null'] = (ets_med['ets']-ets_med['null_ets'])/ets_med['null_ets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_pct_null = weighted_df.subtract(null_med,axis=0).divide(null_med,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_order = null_med.sort_values(ascending=False).index\n",
    "null_gdf = grdc_basins.merge(null_med,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y axis coordinates for each basin\n",
    "basin_y_map = dict(enumerate(basin_order.values))\n",
    "basin_y_rev = {v:3*k for k,v in basin_y_map.items()}\n",
    "\n",
    "# offset for each individual definition within a basin\n",
    "def_y_delta_map = dict(d19=1.25,h17=0.75,ha20=0.25,hm17=-0.25,m19_early=-0.75,m19_low=-1.25)\n",
    "\n",
    "# map basin onto y-coordinate, offset definitions\n",
    "ets_med['basin_y'] = ets_med['basin'].map(basin_y_rev)\n",
    "ets_med['def_y_delta'] = ets_med['definition'].map(def_y_delta_map)\n",
    "ets_med['basin_y'] = ets_med['basin_y']+ets_med['def_y_delta']\n",
    "\n",
    "# colormap for individual products\n",
    "prod_cmap = plt.cm.get_cmap(\"tab20\")\n",
    "prods = list(ets_med['product'].unique())\n",
    "prod_colors = [prod_cmap(i) for i in range(len(prods))]\n",
    "prod_colors[-2] = 'black' # SNOTEL\n",
    "color_dict = dict(zip(prods,prod_colors))\n",
    "\n",
    "# markers for definitions\n",
    "marker_dict =  dict(d19='o',\n",
    "                     h17='X',\n",
    "                     ha20='s',\n",
    "                     hm17='P',\n",
    "                     m19_early='D',\n",
    "                     m19_low='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks',font_scale=2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,20))\n",
    "\n",
    "ax.axvline(0,linestyle='--',color='black') # dashed line indicating no change in forecast skill\n",
    "\n",
    "# add horizontal lines separating basins\n",
    "for k,v in basin_y_rev.items():\n",
    "    ax.axhline(v+1.5,lw=1,color='black')\n",
    "    \n",
    "# sample 3000 estimates from empirical distribution of ensemble mean forecast skill in each basin\n",
    "for idx,row in weighted_pct_null.loc[basin_order].iterrows():\n",
    "    kde = gaussian_kde(row)\n",
    "    X = kde.resample(3000,seed=42)\n",
    "    ax.vlines(X,ymin=basin_y_rev[idx]-1.5,ymax=basin_y_rev[idx]+1.5,color='blue',alpha=0.1,lw=0.2) # add estimates as lines w/ same transparency to approximate conf. int.\n",
    "\n",
    "# add white line indicating median estimate for each basin\n",
    "ax.vlines(weighted_pct_null.median(axis=1).loc[basin_order],ymin=np.arange(0,10)*3-1.5,ymax=np.arange(0,10)*3+1.5,color='white',lw=5)\n",
    "\n",
    "# plot estimates for each data-def combo in each basin, colored by product w/ marker indicating defintiion\n",
    "sns.scatterplot(x='pct_null',y='basin_y',\n",
    "                hue='product',style='definition',\n",
    "                markers=marker_dict,\n",
    "                edgecolor='face',\n",
    "                linewidths=10,\n",
    "                palette=color_dict,\n",
    "                s=100,\n",
    "                data=ets_med,\n",
    "                legend=False,\n",
    "                zorder=3.5,ax=ax)\n",
    "\n",
    "# add SNOTEL on top\n",
    "sns.scatterplot(x='pct_null',y='basin_y',\n",
    "                style='definition',\n",
    "                markers=marker_dict,\n",
    "                edgecolor='black',\n",
    "                linewidth=2,\n",
    "                color=color_dict['SNOTEL'],\n",
    "                s=175,\n",
    "                data=ets_med[ets_med['product']=='SNOTEL'],\n",
    "                legend=False,\n",
    "                zorder=3.5,ax=ax)\n",
    "\n",
    "# y-axis\n",
    "ax.set_yticks(list(basin_y_rev.values())) # center ticks for labels\n",
    "ylab = [basin_ids_rev[i] for i in basin_y_rev.keys()]\n",
    "ylab = [re.sub(\"\\([^)]*\\)\",\"\",s).strip() for s in ylab] # remove alternate names from GRDC naming schemes\n",
    "ynum = [f\"({i})\" for i in range(1,len(ylab)+1)][::-1] # numbers corresponding to inset map\n",
    "ylab = list(map(' '.join, zip(ylab, ynum))) # add numbers to names\n",
    "ax.set_yticklabels(ylab)\n",
    "ax.set_ylabel(\"Basin\")\n",
    "ax.set_ylim(-1.5,len(basin_y_map)*3-1.5)\n",
    "\n",
    "\n",
    "# x-axis\n",
    "ax.set_xlabel(\"% change in forecast skill\\nrelative to null model (T + P only)\")\n",
    "ax.set_xticklabels(np.arange(-200,701,100))\n",
    "\n",
    "# product legend\n",
    "leg1_handles = [Line2D([0],[0],color=prod_colors[i],linestyle='None',marker='o') for i in range(len(prods))]\n",
    "leg = fig.legend(leg1_handles,prods,title='Product',markerscale=2,bbox_to_anchor=(1.475,0.84),ncol=2)\n",
    "\n",
    "# definition legend\n",
    "leg2_handles = [Line2D([0],[0],color='black',linestyle='None',marker=m) for m in marker_dict.values()]\n",
    "leg2_labels = ['D19 (< clim. peak SWE)','H17 (< clim. April 1 SWE)','HA20 (SWEI < -0.8)',\n",
    "               'HM17 (< clim. daily SWE)','M19 Early (< 25 %ile peak date)','M19 Low (< 25 %ile peak SWE)']\n",
    "leg2 = fig.legend(handles=leg2_handles,labels=leg2_labels,bbox_to_anchor=(1.433,0.625),title=\"Definition\",markerscale=2)\n",
    "\n",
    "\n",
    "\n",
    "# create inset map showing baseline skill\n",
    "ax_ins = inset_axes(ax, width=\"35%\",height=\"50%\",\n",
    "                    loc=\"lower right\",\n",
    "                    bbox_to_anchor=(0.08,-0.1,1.5,1.5),\n",
    "                    bbox_transform=ax.transAxes,\n",
    "                    axes_class=geoaxes.GeoAxes,\n",
    "                    axes_kwargs=dict(map_projection=cartopy.crs.PlateCarree()))\n",
    "bounds = grdc_basins.loc[basin_order].geometry.total_bounds\n",
    "ax_ins.set_extent([bounds[0]-1, bounds[2]+1, bounds[1]-1,bounds[3]+1], crs=ccrs.PlateCarree())\n",
    "\n",
    "# colormap for baseline ETS\n",
    "ets_cmap = plt.cm.get_cmap(\"RdYlGn\")\n",
    "ets_colors = [ets_cmap(i) for i in np.linspace(0.2,0.6,5)]\n",
    "ets_cmap = mpl.colors.ListedColormap(ets_colors,name=\"ets\")\n",
    "ets_cmap.set_under(plt.cm.get_cmap(\"RdYlGn\")(0))\n",
    "ets_cmap.set_over(plt.cm.get_cmap(\"RdYlGn\")(0.8))\n",
    "ets_norm = plt.Normalize(vmin=0,vmax=0.5)\n",
    "\n",
    "# plot map of basin-average skill\n",
    "null_gdf.plot(ax=ax_ins, transform=ccrs.PlateCarree(), column='null_ets',edgecolor='black',cmap=ets_cmap,norm=ets_norm)\n",
    "sm = plt.cm.ScalarMappable(cmap=ets_cmap,norm=ets_norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax_ins,pad=0.05, location='bottom', extend=\"max\",panchor=False,anchor=(2.12,0.53),shrink=0.5)\n",
    "cbar.ax.set_xlabel(r\"ETS\")\n",
    "\n",
    "# add coastlines etc.\n",
    "ax_ins.coastlines('10m')\n",
    "ax_ins.add_feature(cartopy.feature.LAND, facecolor=\"grey\",alpha=0.3)\n",
    "ax_ins.add_feature(cartopy.feature.OCEAN,facecolor=\"white\")\n",
    "ax_ins.add_feature(cartopy.feature.LAKES,alpha=0.1,facecolor=\"grey\")\n",
    "\n",
    "# offsets from centroids of basin polygons for plotting numbers\n",
    "xoff = [-3.9,-5.4,-1.2,-0.05,-0.35,-4.4,-1,0,0,-0.4][::-1]\n",
    "yoff = [1,0,0,-0.2,-0.6,-1,0,-2,0,-0.3][::-1]\n",
    "for i, b in enumerate(basin_order[::-1]):\n",
    "    x,y = null_gdf.loc[b].geometry.centroid.xy # centoid of basin\n",
    "    ax_ins.text(x[0]+xoff[i],y[0]+yoff[i],str(i+1),ha='center',va='center') # add number to plot\n",
    "    if b in [89,51,256]: # draw line from centroid to number in small basins where number won't fit\n",
    "        ax_ins.plot([x[0],x[0]+xoff[i]-3],[y[0],y[0]+yoff[i]],color='black',lw=3,zorder=3.5)\n",
    "ax_ins.title.set_text(\"Baseline forecast skill\")\n",
    "\n",
    "# add labels for main plot and inset map\n",
    "fig.text(0.135,0.86,\"a\",fontsize=28,fontweight=\"bold\")\n",
    "fig.text(0.935,0.467,\"b\",fontsize=28,fontweight=\"bold\")\n",
    "\n",
    "plt.savefig(os.path.join(fig_dir,'basin_ets.pdf'),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null model: \n",
    "$$Pr(WSD) = logit^{-1}(\\beta_0 + \\beta_1 T + \\beta_2 P)$$\n",
    "where T is average AMJ temperature and P is cumulative AMJ precipitation.\n",
    "\n",
    "## Individual dataset-definition combination: \n",
    "$$Pr(WSD) = logit^{-1}(\\beta_0 + \\beta_1 T + \\beta_2 P + \\beta_3 SD_{d,p})$$ where $SD_{d,p}$ is a binary variable indicating a snow drought as defined by applying definition $d$ to data product $p$. \n",
    "\n",
    "## Ensemble mean across all dataset-definition combinations (unweighted): \n",
    "$$Pr(WSD) = logit^{-1}(\\beta_0 + \\beta_1 T + \\beta_2 P + \\beta_3\\frac{1}{n_dn_p}\\sum_d\\sum_p SD_{d,p})$$ where $n_d$ and $n_p$ are the numbers of definitions and datasets, respectively. Essentially, the last term is the fraction of all ensemble members that identify a snow drought in a given year.\n",
    "\n",
    "## Ensemble mean across all dataset-definition combinations (weighted): \n",
    "$$Pr(WSD) = logit^{-1}(\\beta_0 + \\beta_1 T + \\beta_2 P + \\beta_3\\frac{1}{n_dn_p}\\sum_d\\sum_p w_{d,p}SD_{d,p})$$ where $w_{d,p}$ is a weight proportional to the forecast skill (as quantified by ETS) of the dataset-definition combination. This gives ensemble members that are better predictors more weight in the ensemble mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arg",
   "language": "python",
   "name": "arg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
